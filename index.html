<!doctype html>
<head>
    <title>Unified Timbre Transfer</title>
    <meta name="viewport" content="width=900, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="index.css">
    <style>
        #audio-comparison {
            margin-top: 20px;
            text-align: center;
        }
        #target-selector {
            margin-bottom: 20px;
        }
        .audio-grid {
            display: grid;
            grid-template-columns: repeat(6, 1fr);
            gap: 10px;
            margin-top: 10px;
            width: 100%;
        }
        .audio-grid div {
            display: flex;
            flex-direction: column;
            align-items: center;
            overflow: hidden;
        }
        .audio-grid strong {
            margin-bottom: 5px;
            font-weight: normal;
        }
        .audio-grid audio {
            max-width: 100%;
            width: 100%;
        }
        #instrument-select {
            padding: 5px;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div id="content">
        <!-- Previous content remains the same -->
        <h1>Unified Timbre Transfer<small> <a href="" target="_blank">[arXiv]</a></small></h1>
        <h2>A Compact Model for Real-Time Multi-Instrument Sound Morphing</h2>
        
        <div id="authors">
            Anonymous Authors
        </div>

        <h3 id="abstract">Abstract</h3>

        <p>Recent advances in transformer-and diffusion-based deep-generative models have significantly impacted the field of music and audio synthesis.
        However, controllable and real-time interactive models, such as those used for timbre transfer in music production,
        remain largely dominated by auto-encoders and generative adversarial networks.
        In pursuit of efficient and flexible timbre morphing and multi-instrument timbre transfer, we propose a simplified modeling approach,
        utilizing an upsampled two-dimensional timbre space in conjunction with engineered and instrument-dependent, excitation signals.
        Our model enables any-to-many timbre transfer with precise control over timbre, pitch, and loudness, while also allowing for seamless interpolation between instruments,
        eliminating the need for separate model training. We achieve performance comparable to specialized models, while supporting 44.1 kHz generation,
        making it highly relevant for the broader creative music community.</p>
        <p></p>

        <br>
        <img src="pipeline-transparent.png">
        <br>
        
        <h3>Background</h3>

        <p>
            We introduce a novel, monophonic, multi-instrument timbre-transfer framework that improves flexibility,
            control, and real-time performance while simplifying the generation process. Inspired by both <a href="https://magenta.tensorflow.org/ddsp" target="_blank">DDSP</a>
            and <a href="https://forum.ircam.fr/projects/detail/rave/" target="_blank">RAVE</a>, our approach uses a generator conditioned on a perceptual timbre space,
            acting as a neural filter that processes an instrument-dependent excitation signal.
        </p>

        <h3>Audio Comparison</h3>
        <div id="audio-comparison">
            <div id="target-selector">
                <label for="instrument-select">Select Target Instrument: </label>
                <select id="instrument-select">
                    <option value="Bassoon">Bassoon</option>
                    <option value="Cello">Cello</option>
                    <option value="Clarinet">Clarinet</option>
                    <option value="Flute">Flute</option>
                    <option value="Horn">Horn</option>
                    <option value="Oboe">Oboe</option>
                    <option value="Saxophone">Saxophone</option>
                    <option value="Trombone">Trombone</option>
                    <option value="Trumpet">Trumpet</option>
                    <option value="Violin">Violin</option>
                </select>
            </div>
            
            <div class="audio-grid" id="audio-grid">
                <div>
                    <strong>Input</strong>
                    <div id="input-audio-container"></div>
                </div>
                <div>
                    <strong>Target</strong>
                    <div id="target-audio-container"></div>
                </div>
                <div>
                    <strong>Proposed</strong>
                    <div id="final-audio-container"></div>
                </div>
                <div>
                    <strong>No Excitation</strong>
                    <div id="no-ex-audio-container"></div>
                </div>
                <div>
                    <strong>No Harmonics</strong>
                    <div id="no-harms-audio-container"></div>
                </div>
                <div>
                    <strong>With Encoder</strong>
                    <div id="w-enc-audio-container"></div>
                </div>
            </div>
        </div>
        
        <script>
        // Audio track configurations
        const audioTracks = [
            { 
                trackName: 'random_track210026_1_violin', 
                sourceInstrument: 'Violin' 
            },
            { 
                trackName: 'random_track210040_1_trumpet', 
                sourceInstrument: 'Trumpet' 
            },
            { 
                trackName: 'random_track210052_4_bassoon', 
                sourceInstrument: 'Bassoon' 
            },
            { 
                trackName: 'random_track210481_3_clarinet', 
                sourceInstrument: 'Clarinet' 
            },
            { 
                trackName: 'random_track210765_2_saxophone', 
                sourceInstrument: 'Saxophone' 
            }
        ];
        
        // Function to create audio elements
        function createAudioElement(src) {
            const audioElement = document.createElement('audio');
            audioElement.controls = true;
            audioElement.preload = 'auto'; // or 'metadata'
            
            const sourceElement = document.createElement('source');
            sourceElement.type = 'audio/wav';
            sourceElement.src = src;
            
            audioElement.appendChild(sourceElement);
            return audioElement;
        }
        
        // Function to update audio tracks
        function updateAudioTracks(selectedInstrument) {
            audioTracks.forEach(track => {
                containers.forEach((containerId, index) => {
                    const container = document.getElementById(containerId);
                    
                    let fullPath = `${basePath}/${settings[index]}/${containerId === 'target-audio-container' ? selectedInstrument : ''}/${track.trackName}${containerId !== 'input-audio-container' ? `_${track.sourceInstrument}_to_${selectedInstrument}` : ''}.wav`;
                    
                    console.group(`Audio Track Debug: ${containerId}`);
                    console.log('Full Path:', fullPath);
                    console.log('Instrument:', selectedInstrument);
                    console.log('Track:', track);
                    
                    fetch(fullPath)
                        .then(response => {
                            console.log('Fetch Response:', response.status, response.statusText);
                            if (!response.ok) {
                                throw new Error('Network response was not ok');
                            }
                            return response.blob();
                        })
                        .then(blob => {
                            console.log('Blob Size:', blob.size);
                            const audioElement = createAudioElement(fullPath);
                            container.appendChild(audioElement);
                        })
                        .catch(error => {
                            console.error('Error loading audio:', error);
                        });
                    
                    console.groupEnd();
                });
            });
        }
        
        // Add event listener to instrument selector
        document.getElementById('instrument-select').addEventListener('change', function() {
            const selectedInstrument = this.value;
            updateAudioTracks(selectedInstrument);
        });
        
        // Initial load
        document.addEventListener('DOMContentLoaded', () => {
            updateAudioTracks('Bassoon');
        });
        </script>
    </div>
</body>
</html>
