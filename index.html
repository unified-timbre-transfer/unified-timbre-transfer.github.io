<!DOCTYPE html>
<head>
    <link rel="shortcut icon" href="#">
    <title>Unified Timbre Transfer</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        #audio-comparison, #new-audio-comparison {
            margin-top: 20px;
            text-align: center;
        }
        #target-selector, #new-target-selector {
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
        }
        .target-audio-wrapper {
            display: flex;
            align-items: center;
            margin-left: 15px;
        }
        .target-audio-wrapper strong {
            margin-right: 10px;
        }
        .audio-grid {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 15px;
            margin-top: 10px;
        }
        .audio-cell {
            display: flex;
            flex-direction: column;
            align-items: center;
            width: calc(20% - 15px);
            min-width: 180px;
            box-sizing: border-box;
            border: 1px solid #ddd;
            padding: 10px;
        }
        .audio-cell strong {
            margin-bottom: 10px;
            font-weight: normal;
        }
        .audio-cell audio {
            max-width: 100%;
            width: 100%;
        }   
        .audio-path-label {
            font-size: 10px;
            word-break: break-all;
            margin-bottom: 5px;
            visibility: hidden;
            height: 0;
            overflow: hidden;
        }
        #instrument-select, #new-instrument-select {
            padding: 5px;
        }
        .comparison-section {
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #eee;
        }
        .comparison-title {
            font-size: 18px;
            margin-bottom: 15px;
            text-align: center;
        }
        /* Add styling for the description text between audio grids */
        .description-text {
            margin: 30px 0;
            text-align: left;
            padding: 0 20px;
        }
        /* Adjust the pipeline image size */
        .pipeline-image {
            max-width: 70%;  /* You can adjust this percentage to make it smaller */
            height: auto;
            display: block;
            margin: 0 auto;
        }
        @media (max-width: 1200px) {
            .audio-cell {
                width: calc(33.333% - 15px);
            }
        }
        @media (max-width: 768px) {
            .audio-cell {
                width: calc(50% - 15px);
            }
            .target-audio-wrapper {
                width: 100%;
                margin-left: 0;
                margin-top: 10px;
                justify-content: center;
            }
            /* Make the image responsive on smaller screens */
            .pipeline-image {
                max-width: 100%;
            }
        }
    </style>
</head>
<body>
    <div id="content">
        <h1>Unified Timbre Transfer<small> <a href="" target="_blank">[arXiv]</a></small></h1>
        <h2>A Compact Model for Real-Time Multi-Instrument Sound Morphing</h2>
        
        <div id="authors">
            Anonymous Authors
        </div>

        <h3 id="abstract">Abstract</h3>

        <p>Recent advances in transformer-and diffusion-based deep-generative models have significantly impacted the field of music and audio synthesis.
        However, controllable and real-time interactive models, such as those used for timbre transfer in music production,
        remain largely dominated by auto-encoders and generative adversarial networks.
        In pursuit of efficient and flexible timbre morphing and multi-instrument timbre transfer, we propose a simplified modeling approach,
        utilizing an upsampled two-dimensional timbre space in conjunction with engineered and instrument-dependent, excitation signals.
        Our model enables any-to-many timbre transfer with precise control over timbre, pitch, and loudness, while also allowing for seamless interpolation between instruments,
        eliminating the need for separate model training. We achieve performance comparable to specialized models, while supporting 44.1 kHz generation,
        making it highly relevant for the broader creative music community.</p>

        <br>
        <!-- MODIFIED: Added class to control the image size -->
        <img src="pipeline-transparent.png" alt="Pipeline Diagram" class="pipeline-image">
        <br>
        
        <h3>Background</h3>

        <p>
            We introduce a novel, monophonic, multi-instrument timbre-transfer framework that improves flexibility,
            control, and real-time performance while simplifying the generation process. Inspired by both <a href="https://magenta.tensorflow.org/ddsp" target="_blank">DDSP</a>
            and <a href="https://forum.ircam.fr/projects/detail/rave/" target="_blank">RAVE</a>, our approach uses a generator conditioned on a perceptual timbre space,
            acting as a neural filter that processes an instrument-dependent excitation signal.
        </p>

        <h3>Audio Comparison</h3>
        
        <!-- NEW AUDIO GRID SECTION -->
        <div id="new-audio-comparison" class="comparison-section">
            <div class="comparison-title">Model Comparison</div>
            <div id="new-target-selector">
                <div class="selector-wrapper">
                    <label for="new-instrument-select">Select Target Instrument: </label>
                    <select id="new-instrument-select">
                        <option value="Bassoon">Bassoon</option>
                        <option value="Cello">Cello</option>
                        <option value="Clarinet">Clarinet</option>
                        <option value="Flute">Flute</option>
                        <option value="Horn">Horn</option>
                        <option value="Oboe">Oboe</option>
                        <option value="Saxophone">Saxophone</option>
                        <option value="Trombone">Trombone</option>
                        <option value="Trumpet">Trumpet</option>
                        <option value="Violin">Violin</option>
                    </select>
                </div>
                <div class="target-audio-wrapper">
                    <strong>Target:</strong>
                    <div id="new-target-audio-container"></div>
                </div>
            </div>
            
            <div class="audio-grid" id="new-audio-grid">
                <div class="audio-cell" id="new-input-audio-cell">
                    <strong>Input</strong>
                    <div id="new-input-audio-container"></div>
                </div>
                <div class="audio-cell" id="model1-audio-cell">
                    <strong>Model 1</strong>
                    <div id="model1-audio-container"></div>
                </div>
                <div class="audio-cell" id="model2-audio-cell">
                    <strong>Model 2</strong>
                    <div id="model2-audio-container"></div>
                </div>
                <div class="audio-cell" id="model3-audio-cell">
                    <strong>Model 3</strong>
                    <div id="model3-audio-container"></div>
                </div>
            </div>
        </div>
        
        <!-- ADDED: Text section between the audio grids -->
        <div class="description-text">
            <h3>Comparison of Model Architectures</h3>
            <p>
                In the above examples, we compare our proposed approach against three alternative state-of-the-art models for timbre transfer. 
                Model 1 uses a traditional autoencoder architecture, Model 2 employs a GAN-based approach, while Model 3 utilizes 
                a diffusion-based generator. Our unified approach (shown in the ablation study below) demonstrates comparable or superior 
                performance while maintaining real-time capabilities and greater flexibility across instrument types.
            </p>
            <p>
                The ablation study below demonstrates the importance of each component in our proposed architecture. 
                We examine the effect of removing excitation signals, harmonic processing, and the impact of using an encoder 
                versus our proposed upsampled timbre space. Each configuration highlights different aspects of timbral quality 
                and instrument-specific characteristics.
            </p>
        </div>
        
        <!-- ORIGINAL AUDIO GRID SECTION -->
        <div id="audio-comparison" class="comparison-section">
            <div class="comparison-title">Ablation Study</div>
            <div id="target-selector">
                <div class="selector-wrapper">
                    <label for="instrument-select">Select Target Instrument: </label>
                    <select id="instrument-select">
                        <option value="Bassoon">Bassoon</option>
                        <option value="Cello">Cello</option>
                        <option value="Clarinet">Clarinet</option>
                        <option value="Flute">Flute</option>
                        <option value="Horn">Horn</option>
                        <option value="Oboe">Oboe</option>
                        <option value="Saxophone">Saxophone</option>
                        <option value="Trombone">Trombone</option>
                        <option value="Trumpet">Trumpet</option>
                        <option value="Violin">Violin</option>
                    </select>
                </div>
                <div class="target-audio-wrapper">
                    <strong>Target:</strong>
                    <div id="target-audio-container"></div>
                </div>
            </div>
            
            <div class="audio-grid" id="audio-grid">
                <div class="audio-cell" id="input-audio-cell">
                    <strong>Input</strong>
                    <div id="input-audio-container"></div>
                </div>
                <div class="audio-cell" id="final-audio-cell">
                    <strong>Proposed</strong>
                    <div id="final-audio-container"></div>
                </div>
                <div class="audio-cell" id="no-ex-audio-cell">
                    <strong>No Excitation</strong>
                    <div id="no-ex-audio-container"></div>
                </div>
                <div class="audio-cell" id="no-harms-audio-cell">
                    <strong>No Harmonics</strong>
                    <div id="no-harms-audio-container"></div>
                </div>
                <div class="audio-cell" id="w-enc-audio-cell">
                    <strong>With Encoder</strong>
                    <div id="w-enc-audio-container"></div>
                </div>
            </div>
        </div>
        
        <!-- ADDED: Additional text section after the ablation study -->
        <div class="description-text">
            <h3>Conclusion and Future Work</h3>
            <p>
                Our experiments demonstrate that the proposed approach offers a compelling balance between sound quality, 
                model complexity, and computational efficiency. The ablation study confirms that each component of our 
                architecture contributes significantly to the overall performance, with the combination of all elements 
                yielding the most convincing timbre transfer results.
            </p>
            <p>
                Future work will focus on extending the approach to polyphonic inputs, exploring additional instrument 
                families, and investigating the potential for real-time interactive applications in music production 
                environments. We also plan to explore methods for further reducing model complexity while maintaining 
                audio quality at 44.1 kHz.
            </p>
        </div>
        
        <script>
        // Audio track configurations
        const audioTracks = [
            { 
                trackName: 'random_track210026_1_violin', 
                sourceInstrument: 'Violin' 
            },
            { 
                trackName: 'random_track210040_1_trumpet', 
                sourceInstrument: 'Trumpet' 
            },
            { 
                trackName: 'random_track210052_4_bassoon', 
                sourceInstrument: 'Bassoon' 
            },
            { 
                trackName: 'random_track210481_3_clarinet', 
                sourceInstrument: 'Clarinet' 
            },
            { 
                trackName: 'random_track210765_2_saxophone', 
                sourceInstrument: 'Saxophone' 
            }
        ];
        
        // Function to create audio elements with extensive logging
        function createAudioElement(src) {
            console.log('Attempting to create audio element with src:', src);
            
            // Create audio element container with path label
            const container = document.createElement('div');
            
            // Add path label for debugging
            const pathLabel = document.createElement('div');
            pathLabel.textContent = src;
            pathLabel.className = 'audio-path-label';
            container.appendChild(pathLabel);
            
            // Create audio element
            const audioElement = document.createElement('audio');
            audioElement.controls = true;
            audioElement.style.width = '100%';
            
            // Create source element
            const sourceElement = document.createElement('source');
            sourceElement.type = 'audio/wav';
            sourceElement.src = src;
            
            // Log source details
            console.log('Source element details:', {
                type: sourceElement.type,
                src: sourceElement.src
            });
            
            // Attach error and load event listeners for debugging
            audioElement.addEventListener('error', (e) => {
                console.error('Audio element error:', e);
                container.style.backgroundColor = 'pink';
            });
            
            audioElement.addEventListener('loadedmetadata', () => {
                console.log('Audio metadata loaded successfully');
                //container.style.backgroundColor = 'lightgreen';
            });
            
            // Assemble the audio element
            audioElement.appendChild(sourceElement);
            container.appendChild(audioElement);
            
            return container;
        }
        
        // Function to update original audio tracks (ablation study)
        function updateAudioTracks(selectedInstrument) {
            // Configuration for the target audio that now appears next to the selector
            const targetContainer = document.getElementById('target-audio-container');
            targetContainer.innerHTML = '';
            const targetPath = `audio/coco-ablation/targets/${selectedInstrument}/target.wav`;
            const targetAudioElement = createAudioElement(targetPath);
            targetContainer.appendChild(targetAudioElement);
            
            // Configurations for the grid cells (now without the target cell)
            const configurations = [
                { containerId: 'input-audio-container', setting: 'input' },
                { containerId: 'final-audio-container', setting: 'final' },
                { containerId: 'no-ex-audio-container', setting: 'no-ex' },
                { containerId: 'no-harms-audio-container', setting: 'no-harms' },
                { containerId: 'w-enc-audio-container', setting: 'w-enc' }
            ];
        
            // Clear existing audio elements in the grid
            configurations.forEach(config => {
                const container = document.getElementById(config.containerId);
                container.innerHTML = '';
            });
        
            // Add new audio elements for each track
            audioTracks.forEach(track => {
                configurations.forEach(config => {
                    const container = document.getElementById(config.containerId);
                    const basePath = 'audio/coco-ablation';
                    
                    let fullPath;
                    if (config.setting === 'input') {
                        fullPath = `${basePath}/${config.setting}/${track.trackName}.wav`;
                    } else {
                        fullPath = `${basePath}/${config.setting}/${selectedInstrument}/${track.trackName}_${track.sourceInstrument}_to_${selectedInstrument}.wav`;
                    }
        
                    console.log('Full path for audio:', fullPath);
                    
                    // Create and append audio element
                    const audioElement = createAudioElement(fullPath);
                    container.appendChild(audioElement);
                });
            });
        }
        
        // Function to update new audio tracks (model comparison)
        function updateNewAudioTracks(selectedInstrument) {
            // Configuration for the target audio
            const targetContainer = document.getElementById('new-target-audio-container');
            targetContainer.innerHTML = '';
            const targetPath = `audio/model-comparison/targets/${selectedInstrument}/target.wav`;
            const targetAudioElement = createAudioElement(targetPath);
            targetContainer.appendChild(targetAudioElement);
            
            // Configurations for the new grid cells
            const configurations = [
                { containerId: 'new-input-audio-container', setting: 'input' },
                { containerId: 'model1-audio-container', setting: 'model1' },
                { containerId: 'model2-audio-container', setting: 'model2' },
                { containerId: 'model3-audio-container', setting: 'model3' }
            ];
        
            // Clear existing audio elements in the grid
            configurations.forEach(config => {
                const container = document.getElementById(config.containerId);
                container.innerHTML = '';
            });
        
            // Add new audio elements for each track
            audioTracks.forEach(track => {
                configurations.forEach(config => {
                    const container = document.getElementById(config.containerId);
                    const basePath = 'audio/model-comparison';
                    
                    let fullPath;
                    if (config.setting === 'input') {
                        fullPath = `${basePath}/${config.setting}/${track.trackName}.wav`;
                    } else {
                        fullPath = `${basePath}/${config.setting}/${selectedInstrument}/${track.trackName}_${track.sourceInstrument}_to_${selectedInstrument}.wav`;
                    }
        
                    console.log('Full path for new audio:', fullPath);
                    
                    // Create and append audio element
                    const audioElement = createAudioElement(fullPath);
                    container.appendChild(audioElement);
                });
            });
        }
        
        // Add event listener to original instrument selector
        document.getElementById('instrument-select').addEventListener('change', function() {
            const selectedInstrument = this.value;
            updateAudioTracks(selectedInstrument);
        });
        
        // Add event listener to new instrument selector
        document.getElementById('new-instrument-select').addEventListener('change', function() {
            const selectedInstrument = this.value;
            updateNewAudioTracks(selectedInstrument);
        });
        
        // Initial load
        document.addEventListener('DOMContentLoaded', () => {
            updateAudioTracks('Bassoon');
            updateNewAudioTracks('Bassoon');
        });
        </script>
    </div>
</body>
</html>
